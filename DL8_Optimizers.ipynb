{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are going to deal with how to do stochastic gradients. There are different type of gradient methods to address the problems arising during optimization. We will cover the different optimizers in the order of their appearance. During that we will uncover the problems they solve and approach with which they solve it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we have a loss function and we want to find its minimum point. Each of the loss functions can be derived as a sum of sample loss functions:\n",
    "\n",
    "\\begin{equation}\n",
    "L_\\Theta\\left(y_t, y_p\\right) = \\frac{1}{n}\\sum^n_i { l_\\Theta\\left(y^{(i)}_t, y^{(i)}_p \\right) }.\n",
    "\\end{equation}\n",
    "\n",
    "The goal is to minimize the $L_\\Theta$ function. This requires to calculate the $l_\\Theta$ for all the samples which can take a lot of time. To avoid that, the common approach is to draw a batch of samples from the training data and calculate the loss (and what is more important the gradient) according to that. The larger the batch, the closer it is to the gradient calculated on the whole dataset. When the batch size becomes smaller the speed of calculating the gradient becomes faster but also noisier. Optimizers are different in the way how they update the parameters according to the gradient. To make it simple what one can do with the optimization process is to accumulate the gradients or change the learning rate adaptively. The research community have found clever ways to do that and now we have different optimizer strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD means Stochastic Gradient Descent. Originally, SGD means that the batch size is one but in practice it has such a high variance that it is not a widespreaded approach, instead we use the Minibatch Gradient Descent, which has the principles we discussed in the previous paragraph. Nowadays, the term SGD means the Minibatch Gradient Descent. But furthermore we think of vanilla-SGD under the term SGD. Vanilla in this context means that the gradients are directly applied on the parameters, there is nothing to do with the learning rate and the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Theta \\leftarrow \\Theta - \\alpha \\cdot \\frac{\\partial L}{\\partial \\Theta} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example for the convergence behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adadelta](https://arxiv.org/pdf/1212.5701.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Overview of optimizers by Ruder](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
