{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Bellmann equation on a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will solve the Bellman-equation on a simple grid world problem which can be seen in the image below. Here are the rules:\n",
    "\n",
    "* The gray areas are fields where the game ends. When we step on them, it is over. \n",
    "* The game can start from any field. \n",
    "* In each step the agent receives reward -1. \n",
    "* On each field the agent can move one step away in the directions indicated by the arrows at left.\n",
    "* The agent can not leave the table. At the edge a step, which would lead beyond the table, won't have effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gridworld](https://drive.google.com/uc?export=download&id=1zXSXzcT1zH1w9111VxJ1F28B7JF2X33n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman-equation as we saw earlier:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi(s) = \\sum_{a, s'}{\\left( r(s, a, s') + \\gamma V^\\pi(s') \\right)T(s, a, s')\\pi(s, a)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $T(s, a, s')$ is the transition matrix\n",
    "* $\\pi(s, a)$ is the policy\n",
    "* $V^\\pi(s')$ is the state-value function\n",
    "* $r(s, a, s')$ is the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a fixed policy $\\pi$, we can calculate the corresponding $V^\\pi(s)$ by solving the Bellman-equation. This can be achieved by using the following iteration:\n",
    "\n",
    "\\begin{equation}\n",
    "V^\\pi_{t+1}(s) = \\sum_{a, s'}{\\left( r(s, a, s') + \\gamma V^\\pi_t(s') \\right) T(s, a, s') \\pi(s, a)}\n",
    "\\end{equation}\n",
    "\n",
    "It can be proved that no metter how the $V^\\pi_0$ is chosen the iteration will converge to the same value $V^\\pi_\\infty$. This process is the **policy evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to $V^\\pi_\\infty$ we calculate another policy $\\pi'$ which can be different from $\\pi$. Then we change $\\pi$ to $\\pi'$ at different $s$ states where the expected return is bigger. This is the **policy improvement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy can be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(s) = argmax_a\\sum_{s'}{\\left( r(s, a, s') + \\gamma V^*_\\infty(s) \\right) T(s, a, s')}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $V^*_\\infty$ is the optimal state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a matrix to store the transition probabilities. (transition matrix)\n",
    "\n",
    "T_sas = np.zeros((16, 16, 4)) # we have 16 states, 4 actions\n",
    "\n",
    "H = np.array([x for x in range(16)])\n",
    "T_sas[H, H - (H % 4 != 0), 0] = 1 # left\n",
    "T_sas[H, H - 4*(H-4 >= 0), 1] = 1 # up\n",
    "T_sas[H, H + ((H+1) % 4 != 0), 2] = 1 # right\n",
    "T_sas[H, H + 4*(H+4 < 16), 3] = 1 # down\n",
    "T_sas[0, :, :] = 0 # in the shaded area each action is pointless\n",
    "T_sas[0, 0, :] = 1\n",
    "T_sas[15, :, :] = 0\n",
    "T_sas[15, 15, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward\n",
    "r_sas = -np.ones((16, 16, 4))\n",
    "\n",
    "r_sas[0, 0, :] = 0\n",
    "r_sas[15, 15, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bellmann-equation\n",
    "def bellman_operator(p_sa, v_s, gamma=0.98):\n",
    "    rT = np.sum(r_sas * T_sas, 1)\n",
    "    rTpi = np.sum(rT * p_sa, 1)\n",
    "    \n",
    "    pi = p_sa.reshape(16, 1, 4).repeat(16, 1)\n",
    "    vT = np.sum(T_sas * pi, 2)\n",
    "    vTpi = np.matmul(vT, v_s) * gamma\n",
    "    return rTpi + vTpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def policy_improvement(v_s, gamma=0.98):\n",
    "    rT = np.sum(r_sas * T_sas, 1)\n",
    "    v = v_s.reshape(1, 16, 1).repeat(16, 0).repeat(4, 2)\n",
    "    pi_idx = np.argmax(rT + gamma * np.sum(T_sas * v, 1), 1)\n",
    "    pi = np.zeros((16, 4))\n",
    "    pi[np.array([x for x in range(16)]), pi_idx] = 1\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value iteration step\n",
    "def value_iteration(gamma=0.98):\n",
    "    rT = np.sum(r_sas * T_sas, 1)\n",
    "    v = v_s.reshape(1, 16, 1).repeat(16, 0).repeat(4, 2)\n",
    "    return np.max(rT + gamma * np.sum(T_sas * v, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy\n",
    "def policy(v_s, gamma=0.98):\n",
    "    rT = np.sum(r_sas * T_sas, 1)\n",
    "    v = v_s.reshape(1, 16, 1).repeat(16, 0).repeat(4, 2)\n",
    "    return np.argmax(rT + gamma * np.sum(T_sas * v, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy evaluation and policy improvement\n",
    "gamma = 0.98\n",
    "v_s = np.zeros(16)\n",
    "p_sa = np.ones((16, 4)) * 0.25 # random policy\n",
    "for _ in range(50):\n",
    "    for _ in range(20): # policy evaluation\n",
    "        v_s = bellman_operator(p_sa, v_s, gamma)\n",
    "    p_sa = policy_improvement(v_s, gamma) # policy improvement\n",
    "pi = policy(v_s, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 0, 0, 3],\n",
       "       [1, 0, 2, 3],\n",
       "       [1, 2, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.reshape((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    , -1.    , -1.98  , -2.9404],\n",
       "       [-1.    , -1.98  , -2.9404, -1.98  ],\n",
       "       [-1.98  , -2.9404, -1.98  , -1.    ],\n",
       "       [-2.9404, -1.98  , -1.    ,  0.    ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_s.reshape((4, 4)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration to solve the problem\n",
    "gamma = 0.9\n",
    "v_s = np.zeros(16)\n",
    "p_sa = np.zeros((16, 4)) * 0.25\n",
    "for _ in range(200):\n",
    "    v_s = bellman_operator(p_sa, v_s, gamma)\n",
    "    p_sa = policy_improvement(v_s, gamma)\n",
    "pi = policy(v_s, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 0, 0, 3],\n",
       "       [1, 0, 2, 3],\n",
       "       [1, 2, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.reshape((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -1.  , -1.9 , -2.71],\n",
       "       [-1.  , -1.9 , -2.71, -1.9 ],\n",
       "       [-1.9 , -2.71, -1.9 , -1.  ],\n",
       "       [-2.71, -1.9 , -1.  ,  0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_s.reshape((4, 4)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the state-values are quite reasonable. They are close to the number of steps required to achieve the closest shaded area. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
