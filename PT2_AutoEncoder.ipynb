{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce and implement auto-encoders. An exaple is shown in the image below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![autoencoder](https://drive.google.com/uc?export=download&id=10sZv98I38nAKqhyVtZPHkmCeeNPgKn6r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoder consists of three parts:\n",
    "\n",
    "1. encoder (higher dimension -(compress)- lower dimension)\n",
    "2. encoded layer \n",
    "3. decoder (lower dimension -(decompress)- higher dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image also shows that the input and the expected output are the same. Therefore an autoencoder approximates an identity function for a specific input space (e.g.: MNIST images). The output of the autoencoder is called the reconstruction of the input and that motivates naming the corresponding loss as reconstruction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder contains bottleneck (in the image the encoded layer) which is smaller then the input. Therefore a successful reocnstruction is possible if the autoencoder manages to find a lower dimensional representation of the input space. This is because to reconstruct the original input at the output requires that all of the information is available for the reconstruction in the decoder. The information flows from layer to layer and it should not loss (at least radically). As a consequence the representation of the input is the most dense in the encoded layer because it is the smallest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond a well-chosen architecture the loss and regularization are very important. There are different types of autoencoders. Basically there are two main groups, the first one is the undercomplete autoencoders and second one is the regularized autoencoders. Of course the second one is wider. Undercomplete autoencoder just uses a narrower encoded layer and a suitable loss function. On the other hand, regularized autoencoder uses a regularization on the *encoded layer*. The type of the used regularization technique differentiate the autoencoder types:\n",
    "\n",
    "* basic autoencoder\n",
    "* sparse autoencoder\n",
    "* contractive autoencoder\n",
    "* denoising autoencoder\n",
    "* variational autoencoder.\n",
    "\n",
    "And of course, it is possible to figure out new ones but these are the most common approaches. Let's go through them then implement the contractive autoencoder and the variational autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic autoencoder\n",
    "\n",
    "Mostly, in the literature basic autoencoder has a one layer encoder and a one layer decoder with sigmoid activations. There is no regularization. Typical losses are Mean Squared Error (MSE) and Binary Cross-entropy (BCE). \n",
    "\n",
    "### Sparse autoencoder\n",
    "\n",
    "A sparse autoencoder tries creating sparse encodings. Sparse means that most of the values in the encodings are low, close to zero while some of them is high (when the neuron is activated). This autoencoder can be good for (for example) creating features for further classifications. Losses: MSE, BCE. For regularization L1 and L2 or the most common one is a Kullback-Leibler divergence based term:\n",
    "\n",
    "\\begin{equation}\n",
    "L_r = \\sum_j^J{KL\\left(\\rho||\\rho_j\\right)} = \\sum_j^J{\\rho \\log{\\frac{\\rho}{\\rho_j}} + (1-\\rho) \\log{\\frac{1-\\rho}{1-\\rho_j}}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\rho_j$ is the average activation of the ($j^{th}$) neuron in the encoded layer, $\\rho$ is a small value (like 0.05) to enforce the unimportant nodes to be low and this forces the sparsity. \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_j = \\frac{1}{I}\\sum_i^I{a_j(x_i)},\n",
    "\\end{equation}\n",
    "\n",
    "here the summation goes over the number of samples in the training data (or batch in case of stochastic descent). \n",
    "\n",
    "### Contractive autoencoder\n",
    "\n",
    "The goal of the contractive autoencoder is to make the autoencoder less sensitive to noise or saying it in an other way, make it locally more resistent to small changes. This can help to achieve better generalization capabilities. Losses are MSE, BCE and others. The regularization term uses the Frobenius-norm of the so called Jacobian-matrix. The Jacobian-matrix shows how the output changes if the input changes a bit and the Frobenius-norm describes this sensitivity with one number. Now the details with some further explanations. Jacobi-matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "J_{ij} = \\frac{\\partial y_i}{\\partial x_j}.\n",
    "\\end{equation}\n",
    "\n",
    "Such a matrix is the gradient of the so called vector-vector functions. As the name suggests in a vector-vector function the input is a vector and the output is a vector. In an autoencoder we have a multidimensional input, which can be represented by a vector, and the output is the activations in the encoded layer, which can be also represented by a vector. The elements of the Jacobian basically shows how the input-output element pairs relates in terms of a derivative. So a row of the matrix shows if we change a pixel in the input image (suppose it is gray scaled and it has only one channel) then how the activation value in each neurons of the encoded layers will change. \n",
    "\n",
    "The Frobenius-norm can be defiened as it induced by a vector norm, specifically the 2-norm. Here is the mathematical definition then the concrete one:\n",
    "\n",
    "\\begin{equation}\n",
    "|| A ||_F = \\sup_v \\frac{|| A v ||_2}{|| v ||_2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "|| A ||_F = \\left( \\sum_{i, j}{a^2_{ij}} \\right)^\\frac{1}{2}\n",
    "\\end{equation}\n",
    "\n",
    "Now the regularization term:\n",
    "\n",
    "\\begin{equation}\n",
    "L_r = \\left( \\sum_{i, j}{\\left(\\frac{\\partial y_i}{\\partial x_j}\\right)^2} \\right)^\\frac{1}{2}.\n",
    "\\end{equation}\n",
    "\n",
    "### Denoising autoencoder\n",
    "\n",
    "The goal of the denoising autoencoder is somewhat similar to the contractive autoencoder's. The denoising autoencoder helps to make the encoder more flexibel against noise and can prevent overfitting because the autoencoder never sees the same input again but a similar one. Here the data set is augmented by applying noise on the input when it is fed into the network. It is not typical to use regularization term as well.\n",
    "\n",
    "### Variational autoencoder\n",
    "\n",
    "This encoder is a generative model. This means that the result of the learning is a model which can be used to generate outputs like the inputs but from noise. So the training goes as in case of the autoencoders but then the encoded layer is not used as a compressed representation but instead we generate random values for the encoded layers and then propagate forward through the decoder. The result should be similar to the samples in the training data sets. (This is implemented in the codes below.) Let's see how does it work! \n",
    "\n",
    "Here the regularization term and the structure of the encoded layer is important. The reconstruction loss can be anything but usually it is MSE or BCE (in the implementation we will use BCE). \n",
    "\n",
    "Now we introduce briefly how VAE works and more theoretical details can be seen at the end of this tutorial. The total loss function in a VAE ($\\overline{x}$ is the output): \n",
    "\n",
    "\\begin{equation}\n",
    "L = BCE(x, \\overline{x}) + KL\\left( q(z|x)||p(z) \\right).\n",
    "\\end{equation}\n",
    "\n",
    "$p(z)$ is chosen as a standard normal distribution ($N(0, 1)$). $q(z|x)$ is given by the encoder. But it is difficult to generate a distribution directly therefore we use normal distribution and the encoder layer approximates the mean and the standard deviation. Both of them is a vector. Basically the encoder has two output vectors, the mean and the standard deviation. \n",
    "\n",
    "Calculating the Kullback-Leibler divergence between normal-distributions can be calculated in a close forme (in the formula $\\mu$, $\\sigma$ is just a scalar):\n",
    "\n",
    "\\begin{equation}\n",
    "KL\\left( N(\\mu, \\sigma) || N(0, 1) \\right) = \\frac{1}{2} (\\mu^2 + \\sigma^2 - 1 - \\log\\sigma^2).\n",
    "\\end{equation}\n",
    "\n",
    "Because we have distribution in the encoded layer we need to sample a vector from $N(\\mu, \\sigma)$ but with a direct sampling the backpropagation would not be possible. Therefore we apply the so called reparametrization trick. So:\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon \\leftarrow N(0, 1) \\\\\n",
    "z = \\mu + \\sigma \\cdot \\epsilon.\n",
    "\\end{align}\n",
    "\n",
    "$z$ is the sampled vector and this will be fed into the decoder part. The image below summarizes the structure of the VAE. $\\mu$ and $\\sigma$ have the same size. \n",
    "\n",
    "![vae](https://drive.google.com/uc?export=download&id=1RXLJTtvUc6OiWddgmVp5QXE4eUjhi_vQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "[Contractive autoencoder](http://www.icml-2011.org/papers/455_icmlpaper.pdf)\n",
    "[Variational autoencoder](https://arxiv.org/pdf/1606.05908.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation first we are dealing with the CAE and two ways how to calculate the Frobenius-norm. The second implementation is the VAE. We will see that the CAE can easily solve the reconstruction but it is hardly able to generate new numbers however the VAE is able to reconstruct and generate new images from noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True # this line is for autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pckutils import mnist, utils\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradients of CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGradCAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f, out_f):\n",
    "        super(AutoGradCAE, self).__init__()\n",
    "        self.in_fetaures = in_f\n",
    "        self.out_features = out_f\n",
    "        \n",
    "        # important to create here all the functions which contain parameters to be trained\n",
    "        # otherwise model.parameters() would return an empty list\n",
    "        self.lin_enc = nn.Linear(self.in_fetaures, self.out_features)\n",
    "        self.lin_dec = nn.Linear(self.out_features, self.in_fetaures)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x.requires_grad=True # this is for taking the gradient according to x as well\n",
    "        self.y_enc = torch.sigmoid(self.lin_enc(x))\n",
    "        self.ae_reg = self.__jacobi(x, self.y_enc)\n",
    "        y_out = torch.sigmoid(self.lin_dec(self.y_enc))\n",
    "        return y_out\n",
    "    \n",
    "    def __jacobi(self, x, y):\n",
    "        j_frobenius = 0.0\n",
    "        for b in range(x.size(0)):\n",
    "            for i in range(self.out_features):\n",
    "                # creat_graph is important because we will calculate the gradient of j_frobenius according to model parameters\n",
    "                gradients = grad(y[b, i], x, retain_graph=True, create_graph=True)[0] \n",
    "                j_frobenius += gradients.pow(2).sum()\n",
    "        return j_frobenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 20), requires_grad=True)\n",
    "bae = AutoGradCAE(20, 5)\n",
    "y = bae(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0986, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bae.ae_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "for p in bae.parameters():\n",
    "    ws.append(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y, x) + bae.ae_reg\n",
    "bae.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of Jacobi term according to W and b\n",
    "w = ws[0]\n",
    "w.requires_grad=True\n",
    "b = ws[1]\n",
    "b.requires_grad=True\n",
    "\n",
    "s = torch.sigmoid(torch.matmul(w, x[0]) + b)\n",
    "ou = torch.matmul(torch.diag(s * (1-s)), w).norm().pow(2) # manual calculation of the Frobenius norm of the Jacobian\n",
    "j_w = grad(ou, w, retain_graph=True)[0]\n",
    "j_b = grad(ou, b, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative according to the decoder W and b parameters\n",
    "sigm_deriv = y[0] * (1-y[0])\n",
    "sigm_jacobi = torch.diag(sigm_deriv)\n",
    "dec_b = torch.matmul(y[0]-x[0], sigm_jacobi) * 2\n",
    "dec_w = torch.ger(dec_b, bae.y_enc[0])\n",
    "\n",
    "# derivative according to the encoder W and b parameters\n",
    "delta = torch.matmul(torch.matmul((y[0]-x[0]), sigm_jacobi), ws[2])\n",
    "enc_b = torch.matmul(delta, torch.diag(bae.y_enc[0] * (1-bae.y_enc[0]))) * 2\n",
    "enc_w = torch.ger(enc_b, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are OK!\n"
     ]
    }
   ],
   "source": [
    "for p, calc_grad in zip(bae.parameters(), [enc_w + j_w, enc_b + j_b, dec_w, dec_b]):\n",
    "    assert torch.le(torch.abs(p.grad - calc_grad), 1e-7).sum() == calc_grad.numel()\n",
    "print('Gradients are OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST example for CAE and VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051 60000 28 28\n",
      "Reading images: [100%]\n",
      "2049 60000\n",
      "Reading labels: [100%]\n",
      "2051 10000 28 28\n",
      "Reading images: [100%]\n",
      "2049 10000\n",
      "Reading labels: [100%]\n"
     ]
    }
   ],
   "source": [
    "data = mnist.load_mnist('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a black and white (binary) image from the grayscale one\n",
    "# normalize it to 0 (black) and 1 (white)\n",
    "# creates train loader to draw batches\n",
    "x_binary = utils.create_binary_image(data.X_train) \n",
    "X = torch.Tensor(x_binary)\n",
    "tensors = TensorDataset(X)\n",
    "trainloader = DataLoader(tensors, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, calculating the gradient with autograd is too time consuming and requires much more memory then it is reasonable. To see this lets try it on the following example with random inputs and measure the time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data (MNIST is difficult to run due to its huge memory requirements)\n",
    "rt = torch.round(torch.rand(128, 100))\n",
    "random_ts = TensorDataset(rt)\n",
    "rand_trainloader = DataLoader(random_ts, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10,   8] loss_reg: 0.003  loss_rec: 158.098\n",
      "Ellapsed time: 15.63380, memory: 48.25\n"
     ]
    }
   ],
   "source": [
    "# create a function for later usage\n",
    "def train_ae(ae, trainloader, lr, beta, device):\n",
    "    ae.device = device\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(ae.parameters(), lr=lr)\n",
    "    running_loss_reg = 0.0\n",
    "    running_loss_rec = 0.0\n",
    "    cntr = 0\n",
    "    \n",
    "    vmem = 0.0\n",
    "    start = time.clock()\n",
    "    for epoch in range(10):\n",
    "        for i, batch in enumerate(trainloader, 1):\n",
    "            cntr += 1\n",
    "            \n",
    "            optimizer.zero_grad()#never forget this! without this the gradients are accumulated therefore learning will slow down\n",
    "            x = batch[0]\n",
    "            x = x.to(device)\n",
    "        \n",
    "            y = ae(x)\n",
    "            loss_reg = beta * ae.ae_reg\n",
    "            loss_rec = criterion(y, x)\n",
    "            loss = loss_reg + loss_rec\n",
    "            loss.backward()\n",
    "            vmem += psutil.virtual_memory()[2]\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss_reg += loss_reg.item()\n",
    "            running_loss_rec += loss_rec.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('[%3d, %3d] loss_reg: %.3f  loss_rec: %.3f' %\n",
    "                (epoch + 1, i, running_loss_reg / 200, running_loss_rec / 200))\n",
    "            running_loss_reg = 0.0\n",
    "            running_loss_rec = 0.0\n",
    "    end = time.clock()\n",
    "\n",
    "    print(\"Ellapsed time: %.5f, memory: %.2f\" %(end - start, vmem/cntr))\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "ae = AutoGradCAE(100, 50).to(device)\n",
    "train_ae(ae, rand_trainloader, 1e-3, 5e-4, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up we need calculating the gradient by the input manually in **batch** mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTautoencoderCAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f, out_f):\n",
    "        super(MNISTautoencoderCAE, self).__init__()\n",
    "        self.in_f = in_f\n",
    "        self.out_f = out_f\n",
    "        \n",
    "        self.lin_enc = nn.Linear(in_f, out_f)\n",
    "        self.lin_dec = nn.Linear(out_f, in_f)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x - batch_size x in_f\n",
    "        '''\n",
    "        y_encoded = torch.sigmoid(self.lin_enc(x))\n",
    "        self.ae_reg = self.jacobi_loss_calc(y_encoded)\n",
    "        y_out = torch.sigmoid(self.lin_dec(y_encoded))\n",
    "        return y_out\n",
    "    \n",
    "    def jacobi_loss_calc(self, y):\n",
    "        sigmoid_der = y * (1-y)\n",
    "        w = list(self.lin_enc.parameters())[0]\n",
    "        sigmoid_der_2 = sigmoid_der**2\n",
    "        w_2 = w**2\n",
    "        return torch.sum(torch.matmul(sigmoid_der_2, w_2))\n",
    "    \n",
    "    def generate_image(self):\n",
    "        x_random = torch.rand(self.out_f)\n",
    "        return torch.sigmoid(self.lin_dec(x_random))\n",
    "    \n",
    "    def generate_image_from_random(self, x_random):\n",
    "        return torch.sigmoid(self.lin_dec(x_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if this simpler form is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((32, 20))*20\n",
    "mae = MNISTautoencoderCAE(20, 5)\n",
    "bae = AutoGradCAE(20, 5)\n",
    "\n",
    "for mae_p, bae_p in zip(mae.parameters(), bae.parameters()):\n",
    "    mae_p.data = bae_p.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_y = mae(x).data\n",
    "bae_y = bae(x).data\n",
    "torch.eq(mae_y, bae_y).sum() == torch.numel(mae_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4723227620124817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae.ae_reg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47232282161712646"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bae.ae_reg.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring ellapsed time during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10,   8] loss_reg: 0.002  loss_rec: 0.099\n",
      "Ellapsed time: 0.07627, memory: 48.30\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "ae = MNISTautoencoderCAE(100, 50).to(device)\n",
    "train_ae(ae, rand_trainloader, 1e-3, 5e-4, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster. If you try it with the MNIST dataset you can experience running out of memory in case of the autograd-based implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train on MNIST and check the quality of encoding by using the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10, 469] loss_reg: 0.190  loss_rec: 1.526\n",
      "Ellapsed time: 112.84297, memory: 48.23\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "ae = MNISTautoencoderCAE(784, 400).to(device)\n",
    "train_ae(ae, trainloader, 1e-3, 5e-4, device)\n",
    "\n",
    "x_test = utils.create_binary_image([data.X_test[12]])\n",
    "y = ae(torch.tensor(x_test[0]).view(1, -1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa2df1ecf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original\n",
    "plt.imshow(x_test[0].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa2cc19fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstructed\n",
    "plt.imshow(y.detach().numpy().reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTautoencoderVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_in, feature_out):\n",
    "        super(MNISTautoencoderVAE, self).__init__()\n",
    "        self.feature_in = feature_in\n",
    "        self.feature_out = feature_out\n",
    "        \n",
    "        self.lin_enc1 = nn.Linear(feature_in, 600)\n",
    "        self.lin_enc2 = nn.Linear(600, 500)\n",
    "        self.lin_enc_mu = nn.Linear(500, feature_out)\n",
    "        self.lin_enc_std = nn.Linear(500, feature_out)\n",
    "        \n",
    "        self.lin_dec1 = nn.Linear(feature_out, 500)\n",
    "        self.lin_dec2 = nn.Linear(500, 600)\n",
    "        self.lin_dec3 = nn.Linear(600, feature_in)\n",
    "        \n",
    "        self.ae_reg = 0.0\n",
    "        self.device = 0.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        x_ = torch.relu(self.lin_enc1(x))\n",
    "        x_ = torch.relu(self.lin_enc2(x_))\n",
    "        mu = torch.tanh(self.lin_enc_mu(x_)) # mean\n",
    "        std = torch.relu(self.lin_enc_std(x_)) + 1e-8 # standard deviation\n",
    "        samples = torch.normal(torch.zeros(mu.size(0), self.feature_out), torch.ones(mu.size(0), self.feature_out)).to(self.device)\n",
    "        self.u = mu + std * samples # reparametrization trick\n",
    "        \n",
    "        # calculate the regularization part\n",
    "        self.ae_reg = self.calculate_reg(mu, std)\n",
    "        \n",
    "        # decoder\n",
    "        y_ = torch.relu(self.lin_dec1(self.u))\n",
    "        y_ = torch.relu(self.lin_dec2(y_))\n",
    "        return torch.sigmoid(self.lin_dec3(y_))\n",
    "    \n",
    "    def calculate_reg(self, mu, std):\n",
    "        kl_div = 0.5 * (std*std + mu*mu - 1.0 - torch.log(std*std))\n",
    "        return kl_div.sum()/mu.size(0) # devide by the batch_size\n",
    "    \n",
    "    def generate_image(self):\n",
    "        x_random = torch.normal(torch.zeros(self.feature_out), torch.ones(self.feature_out)).to(self.device)\n",
    "        y_ = torch.relu(self.lin_dec1(x_random))\n",
    "        y_ = torch.relu(self.lin_dec2(y_))\n",
    "        return torch.sigmoid(self.lin_dec3(y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reparametrization trick we have to draw samples from a standard normal distribution. It is important to draw different samples for each element in the batch. It can be seem unimportant and maybe one can draw only one sample and use it for a batch of $\\mu$ and $\\sigma$. Unfortunately in this case the VAE will hardly learn. The different samples are important to increase the number of elements because it helps to approximate the underlying distribution more smoothly. (Law of large numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10, 469] loss_reg: 0.046  loss_rec: 0.113\n",
      "[ 20, 469] loss_reg: 0.023  loss_rec: 0.077\n",
      "[ 30, 469] loss_reg: 0.023  loss_rec: 0.066\n",
      "[ 40, 469] loss_reg: 0.033  loss_rec: 0.071\n",
      "[ 50, 469] loss_reg: 0.037  loss_rec: 0.067\n",
      "[ 60, 469] loss_reg: 0.068  loss_rec: 0.065\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "mae = MNISTautoencoderVAE(28*28, 400).to(device)\n",
    "mae.device = device\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mae.parameters(), lr=1e-3)\n",
    "running_loss_reg = 0.0\n",
    "running_loss_rec = 0.0\n",
    "cntr = 0\n",
    "\n",
    "for epoch in range(60):\n",
    "    for i, batch in enumerate(trainloader, 1):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = batch[0]\n",
    "        x = x.to(device)\n",
    "        \n",
    "        y = mae(x)\n",
    "        loss_reg = 5e-4*mae.ae_reg\n",
    "        loss_rec = criterion(y, x)\n",
    "        loss = loss_reg + loss_rec\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_reg += loss_reg.item()\n",
    "        running_loss_rec += loss_rec.item()\n",
    "        cntr += 1\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('[%3d, %3d] loss_reg: %.3f  loss_rec: %.3f' %\n",
    "            (epoch + 1, i, running_loss_reg / cntr, running_loss_rec / cntr))\n",
    "        running_loss_reg = 0.0\n",
    "        running_loss_rec = 0.0\n",
    "        cntr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weights\n",
    "weights = list(map(lambda x: x.cpu().detach().numpy(), mae.parameters()))\n",
    "utils.save_parameters(weights, \"weights/Autoencoder.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights if necessary (to try other things for example you do not need to train)\n",
    "weights = utils.load_parameters(\"weights/Autoencoder.json\")\n",
    "mae = MNISTautoencoderCAE(28*28, 400)\n",
    "pms = list(map(torch.from_numpy, weights))\n",
    "\n",
    "for i, p in enumerate(mae.parameters()):\n",
    "    p.data = pms[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking input and output visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = utils.create_binary_image([data.X_test[12]])\n",
    "y = mae(torch.tensor(x_test[0]).view(1, -1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa328f1470>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y.cpu().detach().numpy().reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa32972dd8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aa4ab4bd68>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# variational encoder for generating numbers\n",
    "img = mae.generate_image()\n",
    "plt.imshow(img.cpu().detach().numpy().reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the images are tend to be mimic images from the dataset. But here we just put randomly generated values into the encoded layer. That is why this is a generative model (and not discriminative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivations for VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we know the joint distribution of x and z, p(x, z). Then we can calculate p(x) and p(z) as marginal probabilities by summing out the z and x, respectively. Basically the task of the autoencoder is to learn a mapping between z and x ($x\\rightarrow z$). We do not know the exact formula but we can approximate the distribution. The correct mapping we do not know is $p(z|x)$ and its approximation is $q(z|x)$. Then what we want is to minimize the difference between them. The KL divergence can express this:\n",
    "\n",
    "\\begin{equation}\n",
    "KL(q(z|x)||p(z|x)).\n",
    "\\end{equation}\n",
    "\n",
    "Then take into account the Bayes-rule because we can not calculate the formula above:\n",
    "\n",
    "\\begin{equation}\n",
    "p(z|x) = \\frac{p(x|z)p(z)}{p(x)}.\n",
    "\\end{equation}\n",
    "\n",
    "Substitute this into the KL-divergence above:\n",
    "\n",
    "\\begin{align}\n",
    "KL(q(z|x)||p(z|x)) &= E_q \\log q(z|x) - E_q \\log p(x|z) - E_q \\log p(z) + E_q \\log p(x) \\\\\n",
    "&= -E_q \\log p(x|z) + KL(q(z|x)||p(z)) + \\log p(x).\n",
    "\\end{align}\n",
    "\n",
    "The $\\log p(x)$ does not depend on $q$ and that is the reason for deleting the expectation. The distribution of the input is fixed therefore its gradient does not change. As a consequence, minimizing the KL divergence at the left hand side is the same as minimizing the\n",
    "\n",
    "\\begin{equation}\n",
    "-E_q \\log p(x|z) + KL(q(z|x)||p(z))\n",
    "\\end{equation}\n",
    "\n",
    "function. We assume $p(z)$ is a standard normal distribution. We know nothing about $p(x|z)$ therefore we use an energy based approach for its calculation:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|z) = \\frac{e^{-E(x, z)}}{Z}.\n",
    "\\end{equation}\n",
    "\n",
    "The choice of the energy ($E$) can be aligned with MSE or BCE or other well known loss functions. Finally we got the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "BCEloss(x, \\overline{x}) + KL(q(z|x)||p(z))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence between normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "KL\\left( N(\\mu, \\sigma) || N(0, 1) \\right) &= \\int_{-\\infty}^\\infty{f(x) \\log\\frac{f(x)}{f_0(x)} dx} \\\\\n",
    "f(x) &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\\\\n",
    "f_0(x) &=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then taking the logarithms:\n",
    "\n",
    "\\begin{align}\n",
    "\\log f(x) &= -\\log \\sqrt{2 \\pi} - \\log \\sigma - \\frac{(x - \\mu)^2}{2\\sigma^2} \\\\\n",
    "\\log f_0(x) &= -\\log \\sqrt{2 \\pi} - \\frac{x^2}{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The substraction of the two logarithm eliminates the $-\\log \\sqrt 2\\pi$ part. Then continuing the integration:\n",
    "\n",
    "\\begin{align}\n",
    "KL(.||.) &= \\int_{-\\infty}^\\infty\\left( f(x) \\log f(x) - f(x) \\log f_0(x) \\right) dx \\\\\n",
    "&= \\int_{-\\infty}^\\infty -f(x) \\log \\sigma dx + \\int_{-\\infty}^\\infty f(x) \\left( \\frac{x^2}{2} - \\frac{(x-\\mu)^2}{2\\sigma^2} \\right) dx \\\\\n",
    "&= -\\log \\sigma + \\int_{-\\infty}^\\infty f(x) \\left( \\frac{x^2}{2} - \\frac{(x-\\mu)^2}{2\\sigma^2} \\right) dx.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we taken into account that $\\int_{-\\infty}^\\infty f(x) dx = 1$. Now we reformulate the second term:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x^2}{2} - \\frac{(x-\\mu)^2}{2\\sigma^2} = \\underbrace{\\left( \\frac{1}{2} - \\frac{1}{2\\sigma^2} \\right)x^2}_{A} + \\underbrace{\\frac{\\mu}{\\sigma^2}x}_{B} - \\underbrace{\\frac{\\mu^2}{2\\sigma^2}}_{C}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "A&: \\int_{-\\infty}^\\infty{\\sigma^2 \\left( \\frac{1}{2} - \\frac{1}{2\\sigma^2} \\right) \\frac{x - \\mu + \\mu}{\\sigma^2}f(x)dx} = \\sigma^2 \\left( \\frac{1}{2} - \\frac{1}{2\\sigma^2} \\right) \\left( 1 + \\frac{\\mu^2}{\\sigma^2} \\right) \n",
    "= \\frac{1}{2} \\left( \\sigma^2 + \\mu^2 - 1 - \\frac{\\mu^2}{\\sigma^2} \\right)\\\\\n",
    "B&: \\int_{-\\infty}^\\infty \\frac{\\mu}{\\sigma^2} x f(x)dx = \\frac{\\mu}{\\sigma^2} (x-\\mu+\\mu)f(x)dx = \\frac{\\mu^2}{\\sigma^2} \\\\\n",
    "C&: \\int_{-\\infty}^\\infty -\\frac{\\mu^2}{\\sigma^2} f(x) dx = -\\frac{\\mu^2}{2\\sigma^2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we used the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{x-\\mu}{\\sigma^2}f(x) = \\frac{\\partial f(x)}{\\partial x} \\\\\n",
    "\\int_{-\\infty}^\\infty \\frac{\\partial f(x)}{\\partial x} dx = 0\n",
    "\\end{align}\n",
    "\n",
    "and integration by parts is also necessary to calculate the results above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got:\n",
    "\n",
    "\\begin{equation}\n",
    "KL\\left( N(\\mu, \\sigma) || N(0, 1) \\right) = \\frac{1}{2}\\left( \\mu^2 + \\sigma^2 - 1 - \\log\\sigma^2 \\right)\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
