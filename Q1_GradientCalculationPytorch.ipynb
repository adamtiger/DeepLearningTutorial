{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Jacobi matrix in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we address the question how to calculate the Jacobi matrix elements in Pytorch. The purpose is to show an approach worth following when necessary. Jacobi matrix elements can be intereseting in case of a CAE (Contractive Auto Encoder). This auto encoder requires the Frobenius norm of the Jacobi matrix in the last layer of the encoder part. The motivation behind this is that CAE is an auto encoder that is less sensitive for the small changes in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are dealing with a simple example to show and check how the calculation works. First, lets define the Jacobi matrix in mathematical terms:\n",
    "\n",
    "\\begin{equation}\n",
    "J_{ij} = \\frac{\\partial F_i(x)}{\\partial x_j}\n",
    "\\end{equation}\n",
    "\n",
    "Then the Frobenius norm:\n",
    "\n",
    "\\begin{equation}\n",
    "|J|^2_F = \\sum_{i,j}{J^2_{ij}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x$ is the input vector while $F$ is a function which calculates the output $y$. In our example $F$:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x) = \\sigma\\left( \\underline{\\underline{W}}x\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$ is the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad # this function is for calculating the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[0.1, 0.3, 0.6], [1.5, 0.3, 4.1], [0.2, -0.3, -0.7]], requires_grad=False)\n",
    "x = torch.tensor([2.0, 1.0, 1.0], requires_grad=True) # this is for calculating gradient according to x\n",
    "z = torch.matmul(W, x)\n",
    "y = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = torch.zeros((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0187,  0.0562,  0.1124],\n",
      "        [ 0.0009,  0.0002,  0.0025],\n",
      "        [ 0.0458, -0.0686, -0.1601]])\n"
     ]
    }
   ],
   "source": [
    "# retain_graph = True is important because the gradient should be calculated for all elements of y\n",
    "# grad requires a scalar as output that is the reason for the cycle\n",
    "for i in range(3):\n",
    "    J[i] = grad(y[i], x, retain_graph=True)[0] # [0] is because the result of grad is a tuple\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0486)\n"
     ]
    }
   ],
   "source": [
    "# calculating the Frobenius norm, the torch.norm function does the same but it gives the square root\n",
    "F = J.pow(2).sum()\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the gradient manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0187,  0.0562,  0.1124],\n",
      "        [ 0.0009,  0.0002,  0.0025],\n",
      "        [ 0.0458, -0.0686, -0.1601]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "J_calc = torch.diag(y*(1-y)).matmul(W)\n",
    "print(J_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0486, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "F_calc = J_calc.pow(2).sum()\n",
    "print(F_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.le(torch.abs(F - F_calc), 1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
