{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will see two regularization methods: L1 (Lasso) and L2 (Ridge). The main goal of regularization is to avoid overfitting. In case of a neural network this is very important because we saw earlier that finding a good neural architecture which can solve a problem is difficult. Most of the time it is better to choose an architecture which has such a huge approximation function space that it seems reasonable it can solve our problem. But if we choose a network with too high complexity it can easily overfit our data. Overfitting is harmful in terms of the generalization power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key of the approximation power of a network is the neuron. More neurons generally involves higher approximation power. With generally I mean that the structure of connections among the neurons also matters. \n",
    "\n",
    "Therefore in order to prevent overfitting we should switch off neurons and only keep the most important ones which are just enough to provide a good approximation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different approaches to achieve this but here we will only focus on L1 and L2 regularization. In a later tutorial we will address a much more deep learning specific method, dropout. \n",
    "\n",
    "Both L1 and L2 regularizations are functoins of the weights in the neurons. Basically, they say if the network uses a specific weight then it comes with a cost so better not to use except if it really helps to reduce the value of the loss function. Here are the formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "L^{(reg)}_w(y_{predicted}, y_{target}) = L_w(y_{predicted}, y_{target}) + \\beta \\cdot \\sum_{i,j}{|w_{ij}|},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "L^{(reg)}_w(y_{predicted}, y_{target}) = L_w(y_{predicted}, y_{target}) + \\beta \\cdot \\frac{1}{2}\\sum_{i,j}{w^2_{ij}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is the L1 regularization and the second one is the L2 regularization. $L_w$ can be a loss like MSE. Now, as we can see these formulas, it is easier to understand why the neurons can be switched off. If a $w_{ij}$ becomes zero then the regularization term decreases, this can decrease the overall loss as well $L^{(reg)}_w$. So for the network it is better to keep only the most important neurons to minimize the loss $L_w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see this in a concrete example. We will examine the activations and the weight of the neurons in a network in case of regularization and without regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
